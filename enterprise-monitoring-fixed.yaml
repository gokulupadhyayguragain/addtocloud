---
# Fixed AlertManager Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config-fixed
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      smtp_smarthost: 'smtp.zoho.com:587'
      smtp_from: 'alerts@addtocloud.tech'
      smtp_auth_username: 'alerts@addtocloud.tech'
      smtp_auth_password: 'your-app-password'
      smtp_require_tls: true
    
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'enterprise-alerts'
      routes:
      - match:
          severity: critical
        receiver: 'critical-alerts'
        repeat_interval: 5m
      - match:
          severity: warning
        receiver: 'warning-alerts'
        repeat_interval: 30m
    
    receivers:
    - name: 'enterprise-alerts'
      email_configs:
      - to: 'admin@addtocloud.tech'
        subject: 'AddToCloud Enterprise Alert: {{ .GroupLabels.alertname }}'
        body: |
          Enterprise Alert Notification
          
          Alert: {{ .GroupLabels.alertname }}
          Severity: {{ .CommonLabels.severity }}
          Instance: {{ .CommonLabels.instance }}
          Service: {{ .CommonLabels.service }}
          
          Description: {{ range .Alerts }}{{ .Annotations.description }}{{ end }}
          
          Time: {{ .CommonAnnotations.timestamp }}
          
          Dashboard: https://grafana.addtocloud.tech
          Runbook: https://docs.addtocloud.tech/runbooks
          
          AddToCloud Enterprise Monitoring System
        
    - name: 'critical-alerts'
      email_configs:
      - to: 'admin@addtocloud.tech,oncall@addtocloud.tech'
        subject: 'ðŸš¨ CRITICAL: {{ .GroupLabels.alertname }}'
        body: |
          CRITICAL ALERT - IMMEDIATE ACTION REQUIRED
          
          Alert: {{ .GroupLabels.alertname }}
          Severity: CRITICAL
          Instance: {{ .CommonLabels.instance }}
          Service: {{ .CommonLabels.service }}
          
          Description: {{ range .Alerts }}{{ .Annotations.description }}{{ end }}
          
          This alert requires immediate attention.
          
          Response time SLA: 15 minutes
          Dashboard: https://grafana.addtocloud.tech
          Escalation: Call +1-XXX-XXX-XXXX
          
    - name: 'warning-alerts'
      email_configs:
      - to: 'alerts@addtocloud.tech'
        subject: 'âš ï¸ Warning: {{ .GroupLabels.alertname }}'
        body: |
          Warning Alert
          
          Alert: {{ .GroupLabels.alertname }}
          Severity: Warning
          Instance: {{ .CommonLabels.instance }}
          Service: {{ .CommonLabels.service }}
          
          Description: {{ range .Alerts }}{{ .Annotations.description }}{{ end }}
          
          Please investigate within 1 hour.
          Dashboard: https://grafana.addtocloud.tech
    
    inhibit_rules:
    - source_match:
        severity: 'critical'
      target_match:
        severity: 'warning'
      equal: ['alertname', 'dev', 'instance']
---
# Fixed AlertManager Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager-fixed
  namespace: monitoring
  labels:
    app: alertmanager-fixed
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alertmanager-fixed
  template:
    metadata:
      labels:
        app: alertmanager-fixed
    spec:
      containers:
      - name: alertmanager
        image: prom/alertmanager:v0.25.0
        ports:
        - containerPort: 9093
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
        volumeMounts:
        - name: config
          mountPath: /etc/alertmanager
        - name: storage
          mountPath: /alertmanager
        args:
        - '--config.file=/etc/alertmanager/alertmanager.yml'
        - '--storage.path=/alertmanager'
        - '--web.external-url=http://monitoring.addtocloud.tech/alertmanager'
        - '--cluster.advertise-address=0.0.0.0:9093'
        livenessProbe:
          httpGet:
            path: /-/healthy
            port: 9093
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /-/ready
            port: 9093
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: config
        configMap:
          name: alertmanager-config-fixed
      - name: storage
        emptyDir: {}
---
# AlertManager Service
apiVersion: v1
kind: Service
metadata:
  name: alertmanager-fixed
  namespace: monitoring
  labels:
    app: alertmanager-fixed
spec:
  selector:
    app: alertmanager-fixed
  ports:
  - name: web
    port: 9093
    targetPort: 9093
  type: ClusterIP
---
# Enhanced Prometheus Rules for Enterprise Monitoring
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-enterprise-rules
  namespace: monitoring
data:
  enterprise-rules.yml: |
    groups:
    - name: addtocloud-enterprise-alerts
      rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
          service: "{{ $labels.service }}"
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }}% for service {{ $labels.service }}"
          
      - alert: HighLatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 0.5
        for: 10m
        labels:
          severity: warning
          service: "{{ $labels.service }}"
        annotations:
          summary: "High latency detected"
          description: "95th percentile latency is {{ $value }}s for service {{ $labels.service }}"
          
      - alert: PodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
        for: 5m
        labels:
          severity: critical
          service: "{{ $labels.pod }}"
        annotations:
          summary: "Pod is crash looping"
          description: "Pod {{ $labels.pod }} is restarting frequently"
          
      - alert: DatabaseConnectionFailure
        expr: up{job="postgres"} == 0
        for: 1m
        labels:
          severity: critical
          service: database
        annotations:
          summary: "Database connection failed"
          description: "Cannot connect to PostgreSQL database"
          
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 10m
        labels:
          severity: warning
          service: infrastructure
        annotations:
          summary: "High CPU usage"
          description: "CPU usage is {{ $value }}% on instance {{ $labels.instance }}"
          
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 10m
        labels:
          severity: warning
          service: infrastructure
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value }}% on instance {{ $labels.instance }}"
          
      - alert: DiskSpaceLow
        expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 90
        for: 5m
        labels:
          severity: critical
          service: infrastructure
        annotations:
          summary: "Disk space low"
          description: "Disk usage is {{ $value }}% on instance {{ $labels.instance }}"
          
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
          service: "{{ $labels.job }}"
        annotations:
          summary: "Service is down"
          description: "Service {{ $labels.job }} on instance {{ $labels.instance }} is down"
